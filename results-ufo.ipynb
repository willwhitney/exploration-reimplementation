{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import altair as alt\n",
    "import altair_saver\n",
    "import glob\n",
    "import os\n",
    "import copy\n",
    "alt.data_transformers.disable_max_rows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def personal():\n",
    "    return {\n",
    "        'config': {\n",
    "            'view': {\n",
    "                'height': 300,\n",
    "                'width': 400,\n",
    "            },\n",
    "            'range': {\n",
    "                'category': {'scheme': 'set2'},\n",
    "                'ordinal': {'scheme': 'set2'},\n",
    "            },\n",
    "            'legend': {\n",
    "                'labelLimit': 0,\n",
    "            },\n",
    "            'background': 'white',\n",
    "            'mark': {\n",
    "                'clip': True,\n",
    "            },\n",
    "            'line': {\n",
    "                'size': 3,\n",
    "#                 'opacity': 0.4\n",
    "            },\n",
    "\n",
    "\n",
    "        }\n",
    "    }\n",
    "\n",
    "def publication():\n",
    "    colorscheme = 'set2'\n",
    "    stroke_color = '333'\n",
    "    title_size = 24\n",
    "    label_size = 20\n",
    "    line_width = 5\n",
    "\n",
    "    return {\n",
    "        'config': {\n",
    "            'view': {\n",
    "                'height': 500,\n",
    "                'width': 600,\n",
    "                'strokeWidth': 0,\n",
    "                'background': 'white',\n",
    "            },\n",
    "            'title': {\n",
    "                'fontSize': title_size,\n",
    "            },\n",
    "            'range': {\n",
    "                'category': {'scheme': colorscheme},\n",
    "                'ordinal': {'scheme': colorscheme},\n",
    "            },\n",
    "            'axis': {\n",
    "                'titleFontSize': title_size,\n",
    "                'labelFontSize': label_size,\n",
    "                'grid': False,\n",
    "                'domainWidth': 5,\n",
    "                'domainColor': stroke_color,\n",
    "                'tickWidth': 3,\n",
    "                'tickSize': 9,\n",
    "                'tickCount': 4,\n",
    "                'tickColor': stroke_color,\n",
    "                'tickOffset': 0,\n",
    "            },\n",
    "            'legend': {\n",
    "                'titleFontSize': title_size,\n",
    "                'labelFontSize': label_size,\n",
    "                'labelLimit': 0,\n",
    "                'titleLimit': 0,\n",
    "                'orient': 'top-left',\n",
    "#                 'padding': 10,\n",
    "                'titlePadding': 10,\n",
    "#                 'rowPadding': 5,\n",
    "                'fillColor': '#ffffff88',\n",
    "#                 'strokeColor': 'black',\n",
    "                'cornerRadius': 0,\n",
    "            },\n",
    "            'rule': {\n",
    "                'size': 3,\n",
    "                'color': '999',\n",
    "                # 'strokeDash': [4, 4],\n",
    "            },\n",
    "            'line': {\n",
    "                'size': line_width,\n",
    "#                 'opacity': 0.4\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "\n",
    "alt.themes.register('personal', personal)\n",
    "alt.themes.register('publication', publication)\n",
    "alt.themes.enable('personal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jobs(pattern, subdir='exploration'):    \n",
    "    jobs = glob.glob(f'results/{subdir}/{pattern}')\n",
    "    results = []\n",
    "    for job in jobs:\n",
    "        name = os.path.basename(os.path.normpath(job))\n",
    "        train_data = pd.read_csv(job + '/train.csv')\n",
    "        train_data['test'] = False\n",
    "        test_data = pd.read_csv(job + '/test.csv')\n",
    "        test_data['test'] = True\n",
    "        data = pd.concat([train_data, test_data], sort=False)\n",
    "        data['name'] = name\n",
    "        results.append(data)\n",
    "    df = pd.concat(results, sort=False)\n",
    "    return df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_with_bars(base_chart, y_col, test, extent='ci'):\n",
    "    dummy_chart = base_chart.mark_circle(size=0, opacity=1).encode(\n",
    "        y=f'mean({y_col}):Q',\n",
    "    ).transform_filter(alt.datum.test == test)\n",
    "    mean_chart = base_chart.encode(\n",
    "        y=f'mean({y_col}):Q'\n",
    "    ).transform_filter(alt.datum.test == test)\n",
    "    err_chart = base_chart.encode(\n",
    "        y=f'{y_col}:Q'\n",
    "    ).transform_filter(alt.datum.test == test).mark_errorband(extent=extent)\n",
    "    return dummy_chart + err_chart + mean_chart\n",
    "\n",
    "def make_base_chart(data, title, color):\n",
    "    chart = alt.Chart(data, title=title).mark_line().encode(\n",
    "        x=alt.X('episode', title='Episode'),\n",
    "        color=color,\n",
    "        tooltip=['Algorithm', 'episode']\n",
    "    ).transform_calculate(\n",
    "        has_score=(alt.datum.score > 0.1),\n",
    "    ).transform_window(\n",
    "        sum_novelty='sum(novelty_score)',\n",
    "        frame=[None, 0],\n",
    "        groupby=['name', 'test'],\n",
    "        sort=[{'field': 'episode', 'order': 'ascending'}],\n",
    "    ).transform_window(\n",
    "        sum_score='sum(score)',\n",
    "        frame=[None, 0],\n",
    "        groupby=['name', 'test'],\n",
    "        sort=[{'field': 'episode', 'order': 'ascending'}],\n",
    "    ).transform_window(\n",
    "        count_score='sum(has_score)',\n",
    "        frame=[None, 0],\n",
    "        groupby=['name', 'test'],\n",
    "        sort=[{'field': 'episode', 'order': 'ascending'}],\n",
    "    ).transform_window(\n",
    "        rolling_mean_score='mean(score)',\n",
    "        frame=[-10, 0],\n",
    "        groupby=['name', 'test'],\n",
    "        sort=[{'field': 'episode', 'order': 'ascending'}]\n",
    "    ).transform_window(\n",
    "        rolling_mean_novelty='mean(novelty_score)',\n",
    "        frame=[-10, 0],\n",
    "        groupby=['name', 'test'],\n",
    "        sort=[{'field': 'episode', 'order': 'ascending'}],\n",
    "    )\n",
    "    return chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.themes.enable('publication')\n",
    "jobs = [\n",
    "    load_jobs('arxiv2_grid40_intrinsic_seed*', subdir='intrinsic'),\n",
    "    load_jobs('arxiv2_grid40_slow_seed*', subdir='slow'),\n",
    "    load_jobs('arxiv2_grid40_seed*', subdir='exploration'),\n",
    "    load_jobs('arxiv2_grid40_noopt_seed*', subdir='exploration'),\n",
    "    load_jobs('arxiv2_grid40_noexplore_seed*', subdir='exploration'),\n",
    "]\n",
    "\n",
    "data = pd.concat(jobs, sort=False)\n",
    "data['Algorithm'] = 'Ours: IR + FP + FA + Optimism'\n",
    "data.loc[data['name'].str.contains('noopt'), 'Algorithm'] = 'IR + FP + Fast adaptation'\n",
    "data.loc[data['name'].str.contains('slow'), 'Algorithm'] = 'IR + Factored policies'\n",
    "data.loc[data['name'].str.contains('intrinsic'), 'Algorithm'] = 'Intrinsic reward'\n",
    "data.loc[data['name'].str.contains('noexplore'), 'Algorithm'] = 'No exploration'\n",
    "algorithms = [\n",
    "    'No exploration',\n",
    "    'Intrinsic reward',\n",
    "    'IR + Factored policies',\n",
    "    'IR + FP + Fast adaptation',\n",
    "    'Ours: IR + FP + FA + Optimism'\n",
    "]\n",
    "\n",
    "subset = data\n",
    "subset = subset[(subset['episode'] <= 1000)]\n",
    "chart = make_base_chart(\n",
    "    subset, \n",
    "    title=\"Gridworld 40x40 with reward\", \n",
    "    color=alt.Color('Algorithm', scale=alt.Scale(domain=algorithms)))\n",
    "\n",
    "chart = plot_with_bars(chart, 'rolling_mean_score', test=True)\n",
    "chart.layer[0].encoding.y['scale'] = alt.Scale(domain=[-0.5, 25], nice=False)\n",
    "chart.layer[0].encoding.color['legend'] = alt.Legend(orient='bottom', legendX=300, legendY=100)\n",
    "# altair_saver.save(chart, 'pv100_reward.pdf', method='node')\n",
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.themes.enable('publication')\n",
    "jobs = [\n",
    "    load_jobs('arxiv2_pv100_intrinsic_seed*', subdir='intrinsic'),\n",
    "    load_jobs('arxiv2_pv100_slow_seed*', subdir='slow'),\n",
    "    load_jobs('arxiv2_pv100_seed*', subdir='exploration'),\n",
    "    load_jobs('arxiv2_pv100_noopt_seed*', subdir='exploration'),\n",
    "    load_jobs('arxiv2_pv100_noexplore_seed*', subdir='exploration'),\n",
    "]\n",
    "data = pd.concat(jobs, sort=False)\n",
    "data['Algorithm'] = 'Ours: IR + FP + FA + Optimism'\n",
    "data.loc[data['name'].str.contains('noopt'), 'Algorithm'] = 'IR + FP + Fast adaptation'\n",
    "data.loc[data['name'].str.contains('slow'), 'Algorithm'] = 'IR + Factored policies'\n",
    "data.loc[data['name'].str.contains('intrinsic'), 'Algorithm'] = 'Intrinsic reward'\n",
    "data.loc[data['name'].str.contains('noexplore'), 'Algorithm'] = 'No exploration'\n",
    "algorithms = [\n",
    "    'No exploration',\n",
    "    'Intrinsic reward',\n",
    "    'IR + Factored policies',\n",
    "    'IR + FP + Fast adaptation',\n",
    "    'Ours: IR + FP + FA + Optimism'\n",
    "]\n",
    "\n",
    "subset = data\n",
    "subset = subset[(subset['episode'] <= 1000)]\n",
    "chart = make_base_chart(\n",
    "    subset, \n",
    "    title=\"Point Velocity with reward\", \n",
    "    color=alt.Color('Algorithm', scale=alt.Scale(domain=algorithms)))\n",
    "\n",
    "chart = plot_with_bars(chart, 'rolling_mean_score', test=True)\n",
    "chart.layer[0].encoding.y['scale'] = alt.Scale(domain=[-1, 70], nice=False)\n",
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.themes.enable('publication')\n",
    "jobs = [\n",
    "    load_jobs('arxiv2_grid40_intrinsic_seed*', subdir='intrinsic'),\n",
    "    load_jobs('arxiv2_grid40_slow_seed*', subdir='slow'),\n",
    "    load_jobs('arxiv2_grid40_seed*', subdir='exploration'),\n",
    "    load_jobs('arxiv2_grid40_noopt_seed*', subdir='exploration'),\n",
    "    load_jobs('arxiv2_grid40_noexplore_seed*', subdir='exploration'),\n",
    "]\n",
    "\n",
    "data = pd.concat(jobs, sort=False)\n",
    "data['Algorithm'] = 'Ours: UFO'\n",
    "data.loc[data['name'].str.contains('noopt'), 'Algorithm'] = 'UF only'\n",
    "data.loc[data['name'].str.contains('slow'), 'Algorithm'] = 'U only'\n",
    "data.loc[data['name'].str.contains('intrinsic'), 'Algorithm'] = 'BBE'\n",
    "data.loc[data['name'].str.contains('noexplore'), 'Algorithm'] = 'Undirected exploration'\n",
    "algorithms = [\n",
    "    'Undirected exploration',\n",
    "    'BBE',\n",
    "    'U only',\n",
    "    'UF only',\n",
    "    'Ours: UFO'\n",
    "]\n",
    "\n",
    "subset = data\n",
    "subset = subset[(subset['episode'] <= 1000)]\n",
    "chart = make_base_chart(\n",
    "    subset, \n",
    "    title=\"\", \n",
    "    color=alt.Color('Algorithm', scale=alt.Scale(domain=algorithms)))\n",
    "\n",
    "chart = plot_with_bars(chart, 'rolling_mean_score', test=True)\n",
    "chart.layer[0].encoding.y['scale'] = alt.Scale(domain=[-0.5, 25], nice=False)\n",
    "for layer in chart.layer:\n",
    "    layer.encoding.y['title'] = 'Reward'\n",
    "chart.layer[0].encoding.color['legend'] = alt.Legend(orient='bottom', legendX=300, legendY=100)\n",
    "\n",
    "# altair_saver.save(chart, 'pv100_reward.pdf', method='node')\n",
    "chart_gridworld = chart\n",
    "\n",
    "jobs = [\n",
    "    load_jobs('arxiv2_pv100_intrinsic_seed*', subdir='intrinsic'),\n",
    "    load_jobs('arxiv2_pv100_slow_seed*', subdir='slow'),\n",
    "    load_jobs('arxiv2_pv100_seed*', subdir='exploration'),\n",
    "    load_jobs('arxiv2_pv100_noopt_seed*', subdir='exploration'),\n",
    "    load_jobs('arxiv2_pv100_noexplore_seed*', subdir='exploration'),\n",
    "]\n",
    "data = pd.concat(jobs, sort=False)\n",
    "data['Algorithm'] = 'Ours: UFO'\n",
    "data.loc[data['name'].str.contains('noopt'), 'Algorithm'] = 'UF only'\n",
    "data.loc[data['name'].str.contains('slow'), 'Algorithm'] = 'U only'\n",
    "data.loc[data['name'].str.contains('intrinsic'), 'Algorithm'] = 'BBE'\n",
    "data.loc[data['name'].str.contains('noexplore'), 'Algorithm'] = 'Undirected exploration'\n",
    "algorithms = [\n",
    "    'Undirected exploration',\n",
    "    'BBE',\n",
    "    'U only',\n",
    "    'UF only',\n",
    "    'Ours: UFO'\n",
    "]\n",
    "\n",
    "subset = data\n",
    "subset = subset[(subset['episode'] <= 1000)]\n",
    "chart = make_base_chart(\n",
    "    subset, \n",
    "    title=\"\", \n",
    "    color=alt.Color('Algorithm', scale=alt.Scale(domain=algorithms)))\n",
    "\n",
    "chart = plot_with_bars(chart, 'rolling_mean_score', test=True)\n",
    "chart.layer[0].encoding.y['scale'] = alt.Scale(domain=[-1, 70], nice=False)\n",
    "for layer in chart.layer:\n",
    "    layer.encoding.y['title'] = 'Reward'\n",
    "chart_pv = chart\n",
    "\n",
    "chart = alt.concat(chart_gridworld, chart_pv, spacing=50)\n",
    "# chart = chart.configure(spacing=20)\n",
    "altair_saver.save(chart, 'grid40_pv100_reward_ufo.pdf', method='node')\n",
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.themes.enable('personal')\n",
    "jobs = [\n",
    "    load_jobs('arxiv_grid20*', subdir='intrinsic'),\n",
    "    load_jobs('arxiv_grid20*', subdir='slow'),\n",
    "    load_jobs('arxiv_grid20*', subdir='exploration'),\n",
    "]\n",
    "data = pd.concat(jobs, sort=False)\n",
    "\n",
    "subset = data\n",
    "# subset['replay1M'] = subset['name'].str.contains('replay1M')\n",
    "# subset = subset[subset['eval'] == False]\n",
    "subset = subset[(subset['episode'] <= 300)]\n",
    "chart = alt.Chart(subset, title=\"Gridworld 20x20\", width=400, height=300).mark_line(size=3).encode(\n",
    "    x='episode',\n",
    "    color='name',\n",
    "#     opacity='test',\n",
    "    tooltip=['name', 'episode', 'score', 'novelty_score', 'count_score:Q']\n",
    ").transform_calculate(\n",
    "    has_score=(alt.datum.score > 0.1),\n",
    ").transform_window(\n",
    "    sum_novelty='sum(novelty_score)',\n",
    "    frame=[None, 0],\n",
    "    groupby=['name', 'test'],\n",
    "    sort=[{'field': 'episode', 'order': 'ascending'}],\n",
    ").transform_window(\n",
    "    sum_score='sum(score)',\n",
    "    frame=[None, 0],\n",
    "    groupby=['name', 'test'],\n",
    "    sort=[{'field': 'episode', 'order': 'ascending'}],\n",
    ").transform_window(\n",
    "    count_score='sum(has_score)',\n",
    "    frame=[None, 0],\n",
    "    groupby=['name', 'test'],\n",
    "    sort=[{'field': 'episode', 'order': 'ascending'}],\n",
    ").transform_window(\n",
    "    rolling_mean_score='mean(score)',\n",
    "    frame=[-5, 0],\n",
    "    groupby=['name', 'test'],\n",
    "    sort=[{'field': 'episode', 'order': 'ascending'}],\n",
    ").transform_window(\n",
    "    rolling_mean_novelty='mean(novelty_score)',\n",
    "    frame=[-5, 0],\n",
    "    groupby=['name', 'test'],\n",
    "    sort=[{'field': 'episode', 'order': 'ascending'}],\n",
    ")\n",
    "\n",
    "(\n",
    "    chart.encode(y='rolling_mean_score:Q').transform_filter(alt.datum.test == False) | \\\n",
    "    chart.encode(y='rolling_mean_score:Q').transform_filter(alt.datum.test == True)\n",
    ") & (\n",
    "    chart.encode(y='sum_novelty:Q').transform_filter(alt.datum.test == False) | \\\n",
    "    chart.encode(y='count_score:Q').transform_filter(alt.datum.test == False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = [\n",
    "    load_jobs('grid20*', subdir='intrinsic'),\n",
    "    load_jobs('grid20_slow_real_noflip', subdir='slow'),\n",
    "    load_jobs('grid_optcheck', subdir='exploration'),\n",
    "]\n",
    "data = pd.concat(jobs, sort=False)\n",
    "\n",
    "subset = data\n",
    "# subset['replay1M'] = subset['name'].str.contains('replay1M')\n",
    "# subset = subset[subset['eval'] == False]\n",
    "subset = subset[(subset['episode'] > 2) & (subset['episode'] <= 300)]\n",
    "chart = alt.Chart(subset, title=\"Gridworld 20x20\", width=400, height=300).mark_line(size=3).encode(\n",
    "    x='episode',\n",
    "    color='name',\n",
    "    opacity='test',\n",
    "    tooltip=['name', 'episode', 'score', 'novelty_score', 'count_score:Q']\n",
    ").transform_calculate(\n",
    "    has_score=(alt.datum.score > 0.1),\n",
    ").transform_window(\n",
    "    sum_novelty='sum(novelty_score)',\n",
    "    frame=[None, 0],\n",
    "    groupby=['name', 'test'],\n",
    "    sort=[{'field': 'episode', 'order': 'ascending'}],\n",
    ").transform_window(\n",
    "    sum_score='sum(score)',\n",
    "    frame=[None, 0],\n",
    "    groupby=['name', 'test'],\n",
    "    sort=[{'field': 'episode', 'order': 'ascending'}],\n",
    ").transform_window(\n",
    "    count_score='sum(has_score)',\n",
    "    frame=[None, 0],\n",
    "    groupby=['name', 'test'],\n",
    "    sort=[{'field': 'episode', 'order': 'ascending'}],\n",
    ").transform_window(\n",
    "    rolling_mean_score='mean(score)',\n",
    "    frame=[-5, 0],\n",
    "    groupby=['name', 'test'],\n",
    "    sort=[{'field': 'episode', 'order': 'ascending'}],\n",
    ").transform_window(\n",
    "    rolling_mean_novelty='mean(novelty_score)',\n",
    "    frame=[-5, 0],\n",
    "    groupby=['name', 'test'],\n",
    "    sort=[{'field': 'episode', 'order': 'ascending'}],\n",
    ")\n",
    "\n",
    "(\n",
    "    chart.encode(y='rolling_mean_score:Q').transform_filter(alt.datum.test == False) | \\\n",
    "    chart.encode(y='rolling_mean_score:Q').transform_filter(alt.datum.test == True)\n",
    ") & (\n",
    "    chart.encode(y='sum_novelty:Q').transform_filter(alt.datum.test == False) | \\\n",
    "    chart.encode(y='count_score:Q').transform_filter(alt.datum.test == False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = [\n",
    "    'grid40*',\n",
    "]\n",
    "data = pd.concat([load_jobs(j) for j in jobs], sort=False)\n",
    "\n",
    "subset = data\n",
    "# subset['replay1M'] = subset['name'].str.contains('replay1M')\n",
    "# subset = subset[subset['eval'] == False]\n",
    "subset = subset[(subset['episode'] > 2) & (subset['episode'] <= 1000)]\n",
    "chart = alt.Chart(subset, title=\"Gridworld 40x40\", width=400, height=300).mark_line(size=3).encode(\n",
    "    x='episode',\n",
    "    color='name',\n",
    "    detail='name',\n",
    "    tooltip=['name', 'episode', 'score', 'novelty_score', 'count_score:Q']\n",
    ").transform_calculate(\n",
    "    has_score=(alt.datum.score > 0.1),\n",
    ").transform_window(\n",
    "    sum_novelty='sum(novelty_score)',\n",
    "    frame=[None, 0],\n",
    "    groupby=['name', 'test'],\n",
    "    sort=[{'field': 'episode', 'order': 'ascending'}],\n",
    ").transform_window(\n",
    "    sum_score='sum(score)',\n",
    "    frame=[None, 0],\n",
    "    groupby=['name', 'test'],\n",
    "    sort=[{'field': 'episode', 'order': 'ascending'}],\n",
    ").transform_window(\n",
    "    count_score='sum(has_score)',\n",
    "    frame=[None, 0],\n",
    "    groupby=['name', 'test'],\n",
    "    sort=[{'field': 'episode', 'order': 'ascending'}],\n",
    ").transform_window(\n",
    "    rolling_mean_score='mean(score)',\n",
    "    frame=[-5, 0],\n",
    "    groupby=['name', 'test'],\n",
    "    sort=[{'field': 'episode', 'order': 'ascending'}],\n",
    ").transform_window(\n",
    "    rolling_mean_novelty='mean(novelty_score)',\n",
    "    frame=[-5, 0],\n",
    "    groupby=['name', 'test'],\n",
    "    sort=[{'field': 'episode', 'order': 'ascending'}],\n",
    ")\n",
    "(chart.encode(y='rolling_mean_score:Q').transform_filter(alt.datum.test == True) | \\\n",
    "chart.encode(y='count_score:Q').transform_filter(alt.datum.test == False)) & \\\n",
    "(chart.encode(y='sum_novelty:Q').transform_filter(alt.datum.test == False) | \\\n",
    "chart.encode(y='policy_entropy:Q').transform_filter(alt.datum.test == True)) & \\\n",
    "chart.encode(y='explore_entropy:Q').transform_filter(alt.datum.test == False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = [\n",
    "#     'pv100_noexplore',\n",
    "#     'pv100_sigmoidstretch_clipvalue',\n",
    "#     'pv100_clipvalue',\n",
    "#     'pv100_sigmoidstretch_clipvalue_tupdate10',\n",
    "#     'pv100_clipvalue_tupdate10',\n",
    "#     'pv100_clipvalue_tupdate1',\n",
    "#     'pv100_clipvalue_tupdate10_temp0.1',\n",
    "#     'pv100_testtemp0.3*',\n",
    "#     'pv100replay1M*',\n",
    "    'pv100entropy*',\n",
    "]\n",
    "data = pd.concat([load_jobs(j) for j in jobs], sort=False)\n",
    "\n",
    "subset = data\n",
    "# subset['replay1M'] = subset['name'].str.contains('replay1M')\n",
    "# subset = subset[subset['eval'] == False]\n",
    "subset = subset[(subset['episode'] > 2) & (subset['episode'] <= 1000)]\n",
    "chart = alt.Chart(subset, title=\"Point Velocity\", width=400, height=300).mark_line(size=3).encode(\n",
    "    x='episode',\n",
    "    color='name',\n",
    "    detail='name',\n",
    "    tooltip=['name', 'episode', 'score', 'novelty_score', 'count_score:Q']\n",
    ").transform_calculate(\n",
    "    has_score=(alt.datum.score > 0.1),\n",
    ").transform_window(\n",
    "    sum_novelty='sum(novelty_score)',\n",
    "    frame=[None, 0],\n",
    "    groupby=['name', 'test'],\n",
    "    sort=[{'field': 'episode', 'order': 'ascending'}],\n",
    "\n",
    ").transform_window(\n",
    "    sum_score='sum(score)',\n",
    "    frame=[None, 0],\n",
    "    groupby=['name', 'test'],\n",
    "    sort=[{'field': 'episode', 'order': 'ascending'}],\n",
    "\n",
    ").transform_window(\n",
    "    count_score='sum(has_score)',\n",
    "    frame=[None, 0],\n",
    "    groupby=['name', 'test'],\n",
    "    sort=[{'field': 'episode', 'order': 'ascending'}],\n",
    "\n",
    ").transform_window(\n",
    "    rolling_mean_score='mean(score)',\n",
    "    frame=[-5, 0],\n",
    "    groupby=['name', 'test'],\n",
    "    sort=[{'field': 'episode', 'order': 'ascending'}],\n",
    "\n",
    ").transform_window(\n",
    "    rolling_mean_novelty='mean(novelty_score)',\n",
    "    frame=[-5, 0],\n",
    "    groupby=['name', 'test'],\n",
    "    sort=[{'field': 'episode', 'order': 'ascending'}],\n",
    "\n",
    ")\n",
    "(chart.encode(y='rolling_mean_score:Q').transform_filter(alt.datum.test == True) | \\\n",
    "chart.encode(y='count_score:Q').transform_filter(alt.datum.test == False)) & \\\n",
    "(chart.encode(y='sum_novelty:Q').transform_filter(alt.datum.test == False) | \\\n",
    "chart.encode(y='policy_entropy:Q').transform_filter(alt.datum.test == True)) & \\\n",
    "chart.encode(y='explore_entropy:Q').transform_filter(alt.datum.test == False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = [\n",
    "    'pm_temp0.1*',\n",
    "    'pm_noexplore*',\n",
    "]\n",
    "data = pd.concat([load_jobs(j) for j in jobs], sort=False)\n",
    "\n",
    "subset = data\n",
    "# subset['replay1M'] = subset['name'].str.contains('replay1M')\n",
    "# subset = subset[subset['eval'] == False]\n",
    "subset = subset[(subset['episode'] > 2) & (subset['episode'] <= 300)]\n",
    "chart = alt.Chart(subset, title=\"Point Mass\", width=400, height=300).mark_line(size=3).encode(\n",
    "    x='episode',\n",
    "    color='name',\n",
    "    detail='name',\n",
    "    tooltip=['name', 'episode', 'score', 'novelty_score', 'count_score:Q']\n",
    ").transform_calculate(\n",
    "    has_score=(alt.datum.score > 0.1),\n",
    ").transform_window(\n",
    "    sum_novelty='sum(novelty_score)',\n",
    "    frame=[None, 0],\n",
    "    groupby=['name', 'test'],\n",
    "    sort=[{'field': 'episode', 'order': 'ascending'}],\n",
    "\n",
    ").transform_window(\n",
    "    sum_score='sum(score)',\n",
    "    frame=[None, 0],\n",
    "    groupby=['name', 'test'],\n",
    "    sort=[{'field': 'episode', 'order': 'ascending'}],\n",
    "\n",
    ").transform_window(\n",
    "    count_score='sum(has_score)',\n",
    "    frame=[None, 0],\n",
    "    groupby=['name', 'test'],\n",
    "    sort=[{'field': 'episode', 'order': 'ascending'}],\n",
    "\n",
    ").transform_window(\n",
    "    rolling_mean_score='mean(score)',\n",
    "    frame=[-5, 0],\n",
    "    groupby=['name', 'test'],\n",
    "    sort=[{'field': 'episode', 'order': 'ascending'}],\n",
    "\n",
    ").transform_window(\n",
    "    rolling_mean_novelty='mean(novelty_score)',\n",
    "    frame=[-5, 0],\n",
    "    groupby=['name', 'test'],\n",
    "    sort=[{'field': 'episode', 'order': 'ascending'}],\n",
    "\n",
    ")\n",
    "(chart.encode(y='rolling_mean_score:Q').transform_filter(alt.datum.test == True) | \\\n",
    "chart.encode(y='count_score:Q').transform_filter(alt.datum.test == False)) & \\\n",
    "(chart.encode(y='sum_novelty:Q').transform_filter(alt.datum.test == False) | \\\n",
    "chart.encode(y='policy_entropy:Q').transform_filter(alt.datum.test == False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = [\n",
    "#     'pm_temp0.1*',\n",
    "    'swingup_noexplore_ptemp0.1-0.03',\n",
    "#     'swingup_temp0.1_ptemp0.1-0.03',\n",
    "#     'swingup_temp0.1_ptemp0.1-0.03_pddqn',\n",
    "    'swingup_divergence*',\n",
    "#     'swingup_divergence_pddqn_plr1e-3*',\n",
    "#     'swingup_divergence_plr1e-3*',\n",
    "]\n",
    "data = pd.concat([load_jobs(j) for j in jobs], sort=False)\n",
    "\n",
    "subset = data\n",
    "# subset['replay1M'] = subset['name'].str.contains('replay1M')\n",
    "# subset = subset[subset['eval'] == False]\n",
    "subset = subset[(subset['episode'] > 2) & (subset['episode'] <= 10000)]\n",
    "chart = alt.Chart(subset, title=\"Swingup Sparse\", width=400, height=300).mark_line(size=3).encode(\n",
    "    x='episode',\n",
    "    color='name',\n",
    "    detail='name',\n",
    "    tooltip=['name', 'episode', 'score', 'novelty_score', 'count_score:Q']\n",
    ").transform_calculate(\n",
    "    has_score=(alt.datum.score > 0.1),\n",
    ").transform_window(\n",
    "    sum_novelty='sum(novelty_score)',\n",
    "    frame=[None, 0],\n",
    "    groupby=['name', 'test'],\n",
    "    sort=[{'field': 'episode', 'order': 'ascending'}],\n",
    "\n",
    ").transform_window(\n",
    "    sum_score='sum(score)',\n",
    "    frame=[None, 0],\n",
    "    groupby=['name', 'test'],\n",
    "    sort=[{'field': 'episode', 'order': 'ascending'}],\n",
    "\n",
    ").transform_window(\n",
    "    count_score='sum(has_score)',\n",
    "    frame=[None, 0],\n",
    "    groupby=['name', 'test'],\n",
    "    sort=[{'field': 'episode', 'order': 'ascending'}],\n",
    "\n",
    ").transform_window(\n",
    "    rolling_mean_score='mean(score)',\n",
    "    frame=[-10, 0],\n",
    "    groupby=['name', 'test'],\n",
    "    sort=[{'field': 'episode', 'order': 'ascending'}],\n",
    "\n",
    ").transform_window(\n",
    "    rolling_mean_novelty='mean(novelty_score)',\n",
    "    frame=[-10, 0],\n",
    "    groupby=['name', 'test'],\n",
    "    sort=[{'field': 'episode', 'order': 'ascending'}],\n",
    "\n",
    ")\n",
    "(chart.encode(y='rolling_mean_score:Q').transform_filter(alt.datum.test == True) | \\\n",
    "chart.encode(y='count_score:Q').transform_filter(alt.datum.test == False)) & \\\n",
    "(chart.encode(y='sum_novelty:Q').transform_filter(alt.datum.test == False) | \\\n",
    "chart.encode(y='policy_entropy:Q').transform_filter(alt.datum.test == False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = [\n",
    "    'pv100entropy_temp0.1_ptemp0.1-0.03',\n",
    "    'pv100_novtemp*',\n",
    "#     'pv100_derp',\n",
    "    'pv100_rootcount',\n",
    "]\n",
    "data = pd.concat([load_jobs(j) for j in jobs], sort=False)\n",
    "\n",
    "subset = data\n",
    "# subset['replay1M'] = subset['name'].str.contains('replay1M')\n",
    "# subset = subset[subset['eval'] == False]\n",
    "subset = subset[(subset['episode'] > 2) & (subset['episode'] <= 1000)]\n",
    "chart = alt.Chart(subset, title=\"Does a harder Qex update improve exploration on PV?\", width=400, height=300).mark_line(size=3).encode(\n",
    "    x='episode',\n",
    "    color='name',\n",
    "    detail='name',\n",
    "    tooltip=['name', 'episode', 'score', 'novelty_score', 'count_score:Q']\n",
    ").transform_calculate(\n",
    "    has_score=(alt.datum.score > 0.1),\n",
    ").transform_window(\n",
    "    sum_novelty='sum(novelty_score)',\n",
    "    frame=[None, 0],\n",
    "    groupby=['name', 'test'],\n",
    "    sort=[{'field': 'episode', 'order': 'ascending'}],\n",
    ").transform_window(\n",
    "    sum_score='sum(score)',\n",
    "    frame=[None, 0],\n",
    "    groupby=['name', 'test'],\n",
    "    sort=[{'field': 'episode', 'order': 'ascending'}],\n",
    ").transform_window(\n",
    "    count_score='sum(has_score)',\n",
    "    frame=[None, 0],\n",
    "    groupby=['name', 'test'],\n",
    "    sort=[{'field': 'episode', 'order': 'ascending'}],\n",
    ").transform_window(\n",
    "    rolling_mean_score='mean(score)',\n",
    "    frame=[-5, 0],\n",
    "    groupby=['name', 'test'],\n",
    "    sort=[{'field': 'episode', 'order': 'ascending'}],\n",
    ").transform_window(\n",
    "    rolling_mean_novelty='mean(novelty_score)',\n",
    "    frame=[-5, 0],\n",
    "    groupby=['name', 'test'],\n",
    "    sort=[{'field': 'episode', 'order': 'ascending'}],\n",
    ")\n",
    "(chart.encode(y='rolling_mean_score:Q').transform_filter(alt.datum.test == True) | \\\n",
    "chart.encode(y='count_score:Q').transform_filter(alt.datum.test == False)) & \\\n",
    "(chart.encode(y='sum_novelty:Q').transform_filter(alt.datum.test == False) | \\\n",
    "chart.encode(y='policy_entropy:Q').transform_filter(alt.datum.test == True)) & \\\n",
    "chart.encode(y='explore_entropy:Q').transform_filter(alt.datum.test == False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([\n",
    "    load_jobs('point-mass_noexplore*'),\n",
    "#     load_jobs('point-mass_clipvalue'),\n",
    "    load_jobs('point-mass_clipvalue_exptemp1'),\n",
    "    load_jobs('point-mass_clipvalue_exptemp5'),\n",
    "#     load_jobs('point-mass_sigmoidstretch_clipvalue_exptemp1'),\n",
    "\n",
    "], sort=False)\n",
    "\n",
    "subset = data\n",
    "subset = subset[subset['test'] == False]\n",
    "subset = subset[subset['episode'] <= 1000]\n",
    "chart = alt.Chart(subset, title=\"Can we learn policies faster than baseline?\").mark_line().encode(\n",
    "    x='episode',\n",
    "    y='rolling_mean_score:Q',\n",
    "    color='name',\n",
    "    detail='eval',\n",
    "    tooltip=['episode', 'score', 'novelty_score']\n",
    ").transform_window(\n",
    "    rolling_mean_score='mean(score)',\n",
    "    frame=[-20, 20]\n",
    ").transform_window(\n",
    "    rolling_mean_novelty='mean(novelty_score)',\n",
    "    frame=[-20, 20]\n",
    ")\n",
    "chart + chart.mark_circle().encode().interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([\n",
    "    load_jobs('point-mass_clipvalue_exptemp1'),\n",
    "    load_jobs('point-mass_sigmoidmargin_clipvalue_exptemp1'),\n",
    "    load_jobs('point-mass_sigmoidstretch_clipvalue_exptemp1'),\n",
    "], sort=False)\n",
    "\n",
    "subset = data\n",
    "subset = subset[subset['test'] == False]\n",
    "subset = subset[subset['episode'] <= 1000]\n",
    "chart = alt.Chart(subset, title=\"Does restricting Q range help?\").mark_line().encode(\n",
    "    x='episode',\n",
    "    y='rolling_mean_novelty:Q',\n",
    "    color='name',\n",
    "    detail='eval',\n",
    "    tooltip=['episode', 'score', 'novelty_score']\n",
    ").transform_window(\n",
    "    rolling_mean_score='mean(score)',\n",
    "    frame=[-40, 0]\n",
    ").transform_window(\n",
    "    rolling_mean_novelty='mean(novelty_score)',\n",
    "    frame=[-40, 0]\n",
    ")\n",
    "chart + chart.mark_circle().encode().interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([\n",
    "    load_jobs('swingupsparse*'),\n",
    "\n",
    "], sort=False)\n",
    "\n",
    "subset = data\n",
    "subset = subset[subset['test'] == False]\n",
    "subset = subset[subset['episode'] <= 1000]\n",
    "chart = alt.Chart(subset, title=\"Cartpole Swingup\").mark_line().encode(\n",
    "    x='episode',\n",
    "    y='rolling_mean_score:Q',\n",
    "    color='name',\n",
    "    detail='eval',\n",
    "    tooltip=['episode', 'score', 'novelty_score']\n",
    ").transform_window(\n",
    "    rolling_mean_score='mean(score)',\n",
    "    frame=[-40, 0]\n",
    ").transform_window(\n",
    "    rolling_mean_novelty='mean(novelty_score)',\n",
    "    frame=[-40, 0]\n",
    ")\n",
    "chart + chart.mark_circle().encode().interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = [\n",
    "    'pv100_clipvalue',\n",
    "    'pv100_clipvalue_tupdate10',\n",
    "    'pv100_clipvalue_tupdate1',\n",
    "]\n",
    "data = pd.concat([load_jobs(j) for j in jobs], sort=False)\n",
    "\n",
    "subset = data\n",
    "subset = subset[subset['test'] == False]\n",
    "subset = subset[subset['episode'] <= 1000]\n",
    "chart = alt.Chart(subset, title=\"Do faster target updates help exploration? Not really.\").mark_line().encode(\n",
    "    x='episode',\n",
    "    y='sum_novelty:Q',\n",
    "    color='name',\n",
    "    detail='eval',\n",
    "    tooltip=['episode', 'score', 'novelty_score']\n",
    ").transform_window(\n",
    "    sum_novelty='sum(novelty_score)',\n",
    "    frame=[None, 0],\n",
    "    groupby=['name']\n",
    ").transform_window(\n",
    "    rolling_mean_score='mean(score)',\n",
    "    frame=[-5, 0],\n",
    "    groupby=['name']\n",
    ").transform_window(\n",
    "    rolling_mean_novelty='mean(novelty_score)',\n",
    "    frame=[-5, 0],\n",
    "    groupby=['name']\n",
    ")\n",
    "\n",
    "chart + chart.mark_circle().encode().interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = [\n",
    "    'pv100_sigmoidstretch_clipvalue',\n",
    "    'pv100_clipvalue',\n",
    "    'pv100_sigmoidstretch_clipvalue_tupdate10',\n",
    "    'pv100_clipvalue_tupdate10',\n",
    "]\n",
    "data = pd.concat([load_jobs(j) for j in jobs], sort=False)\n",
    "\n",
    "subset = data\n",
    "subset = subset[subset['test'] == False]\n",
    "subset = subset[subset['episode'] <= 1000]\n",
    "chart = alt.Chart(subset, title=\"Are sigmoid networks better? Not really, for PV.\").mark_line().encode(\n",
    "    x='episode',\n",
    "    y='sum_novelty:Q',\n",
    "    color='name',\n",
    "    detail='eval',\n",
    "    tooltip=['episode', 'score', 'novelty_score']\n",
    ").transform_window(\n",
    "    sum_novelty='sum(novelty_score)',\n",
    "    frame=[None, 0],\n",
    "    groupby=['name']\n",
    ").transform_window(\n",
    "    rolling_mean_score='mean(score)',\n",
    "    frame=[-5, 0],\n",
    "    groupby=['name']\n",
    ").transform_window(\n",
    "    rolling_mean_novelty='mean(novelty_score)',\n",
    "    frame=[-5, 0],\n",
    "    groupby=['name']\n",
    ")\n",
    "\n",
    "chart + chart.mark_circle().encode().interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([\n",
    "    load_jobs('pv100_sigmoidstretch_clipvalue'),\n",
    "    load_jobs('pv100_clipvalue'),\n",
    "    load_jobs('pv100_sigmoidstretch_clipvalue_tupdate10'),\n",
    "    load_jobs('pv100_noexplore'),\n",
    "\n",
    "], sort=False)\n",
    "\n",
    "subset = data\n",
    "subset = subset[subset['test'] == False]\n",
    "subset = subset[subset['episode'] <= 1000]\n",
    "chart = alt.Chart(subset, title=\"Point Velocity\").mark_line().encode(\n",
    "    x='episode',\n",
    "    y='score:Q',\n",
    "    color='name',\n",
    "    detail='eval',\n",
    "    tooltip=['episode', 'score', 'novelty_score']\n",
    ").transform_window(\n",
    "    rolling_mean_score='mean(score)',\n",
    "    frame=[-40, 0],\n",
    "    groupby=['name']\n",
    ").transform_window(\n",
    "    rolling_mean_novelty='mean(novelty_score)',\n",
    "    frame=[-40, 0],\n",
    "    groupby=['name']\n",
    ")\n",
    "chart + chart.mark_circle().encode().interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
